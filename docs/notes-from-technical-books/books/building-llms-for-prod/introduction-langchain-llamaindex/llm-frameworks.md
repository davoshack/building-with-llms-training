# LLM Frameworks

We have introduced and discussed the core idea behind RAG throughout this book. But before we get into practical examples, we will introduce two commonly used frameworks that are often used to help build RAG projects and LLM pipelines more generally: LangChain and LlamaIndex.  RAG pipelines can be built from scratch without the need for these frameworks. However, frameworks like this help you get a head start and reduce your development time and cost. They typically offer a quicker path to deploying a solution with default configurations. These libraries experiment with various settings and combinations to deliver a ready-to-use, effective solution, all without requiring significant time or effort. They are perfect for reducing the complexities of selecting models or worrying about the language of prompt templates for different tasks. The open-source nature of these libraries further ensures that their methods are tested and effective. They offer the convenience of experimenting with different models through a simple code alteration, personalizing prompt templates, or managing outputs.

There are some downsides to taking advantage of this because you introduce some extra dependencies on external libraries and have to be wary of library updates and framework changes. You can also add some extra flexibility and customization to your specific RAG use case by building from scratch. That said, we still highly recommend using LangChain or LlamaIndex to build your project; you can always switch out modules for more custom code later if it adds improvements to your project.

There are many other LLM pipeline features available to use with these frameworks, like prompting templates, selectors, parsers, indexes, retrievers, data ingestion methods, text splitters, tools, and more